#This session will be heaviliy relying on exteranl guides.
# XGBOOST is a machine learning model with multiple parameters of a gradient boosting algorithm. It fts finance topics well.
# A gradient boosting algorithm gives a prediction model in the form of an ensemble of weak prediction models (WIKIPEDIA)
#Parameters introduced from Analytics Vidhya (https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
" 1.booster [default=gbtree]
Select the type of model to run at each iteration. It has 2 options:
gbtree: tree-based models
gblinear: linear models
2. silent [default=0]
Silent mode is activated is set to 1, i.e., no running messages will be printed.
It’s generally good to keep it 0 as the messages might help in understanding the model.
3. nthread [default to the maximum number of threads available if not set]
This is used for parallel processing, and the number of cores in the system should be entered
If you wish to run on all cores, the value should not be entered, and the algorithm will detect it automatically."


#Import libraries:
import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn import metrics   #Additional scklearn functions
from sklearn.model_selection import GridSearchCV
import matplotlib.pylab as plt

from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 12, 4
dtrain = pd.read_csv('american_bankruptcy.csv', encoding='ISO-8859–1')
target = 'status_label'
IDcol = 'company_name'
def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):
    
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)
        alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain['status_label'],eval_metric='auc')
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])
    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]
    #Print model report:
    print ("\nModel Report")
    print ("Accuracy : %.4g" % metrics.accuracy_score( dtrain ['status_label'].values, dtrain_predictions))
    print ("AUC Score (Train): %f" % metrics.roc_auc_score(dtrain['status_label'], dtrain_predprob))
                    
    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)
    feat_imp.plot(kind='bar', title='Feature Importances')
    plt.ylabel('Feature Importance Score')
        
   
predictors = [x for x in dtrain.columns if x not in [target, IDcol]]
xgb1 = XGBClassifier(
 learning_rate =0.1,
 n_estimators=1000,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 seed=27)
modelfit(xgb1, dtrain, predictors)

# Unfortunately, due to the data base is too generalzied, XGBoost could not provided report.
#if you still want to learn it, please check: https://github.com/wangy8989/Bankruptcy-Prediction-using-Machine-Learning/tree/master
I will keep on learning that and update. Merry Christmas!

